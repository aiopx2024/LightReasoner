"""
==============================================================
 LightR å¾®è°ƒè„šæœ¬
==============================================================


æœ¬è„šæœ¬æŠŠæ•°æ®é›†ã€LoRAã€Trainer ä¸è®­ç»ƒå¾ªç¯æ•´åˆä¸ºä¸€æ¡æµæ°´çº¿ï¼Œç”¨å¯¹æ¯”è½¯æ ‡ç­¾å®Œæˆå¾®è°ƒã€‚

âš ï¸ é‡è¦æç¤ºï¼š
è¿è¡Œå‰è¯·åœ¨é…ç½®åŒºå®Œæˆä»¥ä¸‹æ›¿æ¢ï¼š
    - å°† <path_to_expert_model> æ¢æˆä½ çš„åŸºç¡€æ¨¡å‹è·¯å¾„
      ï¼ˆå¦‚ "Qwen/Qwen2.5-Math-7B" æˆ–æœ¬åœ°æ–‡ä»¶å¤¹ï¼‰ã€‚
    - å°† <path_to_training_dataset> æ¢æˆé‡‡æ ·å¾—åˆ°çš„ JSONL æ•°æ®é›†ã€‚
    - å°† <output_directory> æ¢æˆä¿å­˜æ£€æŸ¥ç‚¹ä¸æœ€ç»ˆæ¨¡å‹çš„ç›®å½•ã€‚
    - æ ¹æ®ç¡¬ä»¶è®¾ç½® torch_dtype
      ï¼ˆä¾‹å¦‚ H100 ä½¿ç”¨ torch.bfloat16ï¼ŒA100 ä½¿ç”¨ torch.float16ï¼‰ã€‚

==============================================================
 è¿è¡Œæ–¹å¼
==============================================================

å‰å°ç›´æ¥è¿è¡Œï¼š
    python LightR_finetuning.py

åå°è®°å½•æ—¥å¿—ï¼ˆé•¿æ—¶é—´è®­ç»ƒæ¨èï¼‰ï¼š
    nohup python LightR_finetuning.py > finetune.log 2>&1 &

å®æ—¶æŸ¥çœ‹æ—¥å¿—ï¼š
    tail -f finetune.log

è®­ç»ƒå®Œæˆåï¼Œå¾®è°ƒæ¨¡å‹ä¼šä¿å­˜åœ¨ï¼š
    <output_directory>   ï¼ˆå³é…ç½®ä¸­è®¾å®šçš„è·¯å¾„ï¼‰

==============================================================
"""


# ================================
# å¾®è°ƒæ­¥éª¤ 1
# ================================
import torch
from torch.utils.data import Dataset
import json
from transformers import AutoTokenizer, AutoModelForCausalLM

class ContrastiveSoftLabelDataset(Dataset):
    def __init__(self, jsonl_path, tokenizer, model_vocab_size, max_length=512):
        self.data = []
        self.tokenizer = tokenizer
        self.vocab_size = model_vocab_size
        self.max_length = max_length

        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                self.data.append(item)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        assistant_prefix = item["prefix"]
        token_ids = item["token_ids"]
        weights = item["weights"]
        question = item["prompt_id"]

        # åº”ç”¨èŠå¤©æ¨¡æ¿æ„å»ºç»“æ„åŒ–è¾“å…¥
        messages = [
            {"role": "system", "content": "Please reason step by step."},
            {"role": "user", "content": question}
        ]
        formatted = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        full_input = formatted + assistant_prefix

        encoding = self.tokenizer(
            full_input,
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].squeeze(0)
        attention_mask = encoding["attention_mask"].squeeze(0)

        labels = torch.zeros(self.vocab_size, dtype=torch.float)
        for tid, weight in zip(token_ids, weights):
            if tid < self.vocab_size:
                labels[tid] = weight

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }


# ================================
# å¾®è°ƒæ­¥éª¤ 2
# ================================
from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM as _AutoModelForCausalLM

def load_lora_model(model_path: str, torch_dtype, device_map="auto"):
    base_model = _AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch_dtype,
        device_map=device_map
    )

    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )

    return get_peft_model(base_model, lora_config)


# ================================
# å¾®è°ƒæ­¥éª¤ 3
# ================================
import torch.nn.functional as F
from transformers import Trainer

class SoftLabelKLTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        logits = model(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"]
        ).logits

        vocab_size = inputs["labels"].size(-1)
        logits = logits[:, -1, :vocab_size]  # å½¢çŠ¶ä¸º [batch_size, vocab_size]

        log_probs = F.log_softmax(logits, dim=-1)
        soft_labels = inputs["labels"]

        loss = F.kl_div(log_probs, soft_labels, reduction="batchmean")
        return loss


# ================================
# å¾®è°ƒæ­¥éª¤ 4ï¼ˆä¸»è®­ç»ƒæµç¨‹ï¼‰
# ================================
from transformers import TrainingArguments


# === é…ç½®ï¼ˆè¿è¡Œå‰è¯·å…ˆä¿®æ”¹ï¼‰ ===

# æ¨¡å‹è·¯å¾„
model_path = "<path_to_expert_model>"           # ä¾‹å¦‚ "Qwen/Qwen2.5-Math-7B" æˆ–æœ¬åœ°ç›®å½•

# æ•°æ®é›†ä¸è¾“å‡º
dataset_path = "<path_to_training_dataset>"     # ä¾‹å¦‚ "./cd_dist_samples_gsm8k.jsonl"
output_dir   = "<output_directory>"             # ä¾‹å¦‚ "./finetuned_qwen2.5_cd_gsm8k"

# è®¾å¤‡ä¸ç²¾åº¦
torch_dtype = "<torch_dtype>"                   # ä¾‹å¦‚ H100 ç”¨ torch.bfloat16ï¼ŒA100 ç”¨ torch.float16

# è®­ç»ƒè¶…å‚æ•°
batch_size = 8                                  # å•å¡æ‰¹å¤§å°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
gradient_accumulation_steps = 2                 # å¢å¤§ä¼šæ¨¡æ‹Ÿæ›´å¤§çš„æœ‰æ•ˆæ‰¹é‡
eval_steps = 200                                # æ¯ N æ­¥æ‰§è¡Œä¸€æ¬¡è¯„ä¼°
save_steps = 200                                # æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
logging_steps = 10                              # æ¯ N æ­¥è®°å½•æ—¥å¿—
max_steps = 1000                                # æ ¹æ®å®éªŒè®¾ç½®çš„è®­ç»ƒæ­¥æ•°
lr = 5e-5                                       # å­¦ä¹ ç‡


# === é’ˆå¯¹ H100 çš„å…¨å±€ä¼˜åŒ– ===
torch.set_float32_matmul_precision("high")

# === åŠ è½½åˆ†è¯å™¨ä¸æ•°æ®é›† ===
tokenizer = AutoTokenizer.from_pretrained(model_path)
vocab_size = tokenizer.vocab_size
train_dataset = ContrastiveSoftLabelDataset(dataset_path, tokenizer, model_vocab_size=vocab_size)

# === åŠ è½½åº”ç”¨ LoRA çš„æ¨¡å‹ ===
model = load_lora_model(
    model_path=model_path,
    torch_dtype=torch_dtype,
    device_map="auto"
)

# === æ•°æ®æ•´ç†å‡½æ•° ===
def collate_fn(batch):
    return {
        "input_ids": torch.nn.utils.rnn.pad_sequence(
            [x["input_ids"] for x in batch], batch_first=True, padding_value=tokenizer.pad_token_id
        ),
        "attention_mask": torch.nn.utils.rnn.pad_sequence(
            [x["attention_mask"] for x in batch], batch_first=True, padding_value=0
        ),
        "labels": torch.stack([x["labels"] for x in batch])
    }

# === è®­ç»ƒå‚æ•° ===
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    learning_rate=lr,
    max_steps=max_steps,
    logging_steps=logging_steps,
    save_steps=save_steps,
    save_total_limit=2,
    bf16=True,
    fp16=False,
    report_to="none",
    disable_tqdm=False,
    remove_unused_columns=False
)

# === è®­ç»ƒå™¨ ===
trainer = SoftLabelKLTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
    data_collator=collate_fn
)

# === å¼€å§‹è®­ç»ƒ ===
if __name__ == "__main__":
    print("ğŸš€ Starting full fine-tuning on GSM8K contrastive samples...")
    trainer.train()
    print("âœ… Fine-tuning complete!")
