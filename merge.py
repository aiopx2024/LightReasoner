'''ç”¨äºåœ¨æœ¬åœ°åˆå¹¶å®Œæ•´æ¨¡å‹ï¼ˆåŸºåº§ + LoRAï¼‰ï¼Œè®©å…¶æ— éœ€ LoRA ä¾èµ–å³å¯ç‹¬ç«‹ä½¿ç”¨ã€‚'''


import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# === é…ç½® ===
base_model_path = "<path_to_base_model>" # ä¾‹å¦‚ ./Qwen2.5-Math-7B
lora_ckpt_path = "<path_to_lora_checkpoint>" # ä¾‹å¦‚ ./ft_qwen2.5_gsm8k/checkpoint-1000
merged_model_path = "<path_to_save_merged_model>" # ä¾‹å¦‚ ./ft-7B-merged

# === ç¬¬ 1 æ­¥ï¼šåŠ è½½åŸºåº§æ¨¡å‹ä¸ LoRA é€‚é…å™¨ ===
print("ğŸ”§ Loading base + LoRA model...")
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
model = PeftModel.from_pretrained(base_model, lora_ckpt_path)

# === ç¬¬ 2 æ­¥ï¼šåˆå¹¶æƒé‡å¹¶å¸è½½ PEFT ç»“æ„ ===
print("ğŸ”— Merging LoRA weights into base model...")
merged_model = model.merge_and_unload()

# === ç¬¬ 3 æ­¥ï¼šä¿å­˜åˆå¹¶åçš„æ¨¡å‹ä¸åˆ†è¯å™¨ ===
print(f"ğŸ’¾ Saving merged model to: {merged_model_path}")
merged_model.save_pretrained(merged_model_path)

print("ğŸ’¾ Saving tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(base_model_path)
tokenizer.save_pretrained(merged_model_path)


print("âœ… Merge complete! Model saved locally.")
